# å¼€æºä»£ç æ¨¡å‹æŒ‡å—

å¼€æºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è®©ä½ å¯ä»¥åœ¨æœ¬åœ°éƒ¨ç½²ã€å®šåˆ¶å’Œä½¿ç”¨ AI ç¼–ç¨‹èƒ½åŠ›ï¼Œä¿æŠ¤éšç§ä¸”å¯æ§ã€‚

## ğŸ¯ ä¸ºä»€ä¹ˆé€‰æ‹©å¼€æºæ¨¡å‹ï¼Ÿ

### ä¼˜åŠ¿
âœ… **éšç§ä¿æŠ¤**: æ•°æ®ä¸ç¦»å¼€æœ¬åœ°
âœ… **æˆæœ¬æ§åˆ¶**: æ—  API è´¹ç”¨
âœ… **å¯å®šåˆ¶**: å¯ä»¥ fine-tune
âœ… **ç¦»çº¿ä½¿ç”¨**: æ— éœ€äº’è”ç½‘
âœ… **é€æ˜æ€§**: æ¨¡å‹æ¶æ„å…¬å¼€

### åŠ£åŠ¿
âš ï¸ **ç¡¬ä»¶è¦æ±‚**: éœ€è¦å¼ºå¤§çš„ GPU
âš ï¸ **æ€§èƒ½å·®è·**: ç›¸æ¯” GPT-4/Claude æœ‰å·®è·
âš ï¸ **è®¾ç½®å¤æ‚**: éœ€è¦æŠ€æœ¯çŸ¥è¯†
âš ï¸ **ç»´æŠ¤æˆæœ¬**: éœ€è¦è‡ªå·±ç®¡ç†å’Œæ›´æ–°

---

## ğŸ† ä¸»æµå¼€æºä»£ç æ¨¡å‹

| æ¨¡å‹ | å¼€å‘è€… | å‚æ•°é‡ | ç‰¹è‰² | ç¡¬ä»¶è¦æ±‚ |
|------|--------|--------|------|---------|
| **DeepSeek Coder** | DeepSeek | 6.7B - 33B | ä¸­æ–‡å‹å¥½ | 16GB+ |
| **CodeLlama** | Meta | 7B - 34B | ç”Ÿæ€æˆç†Ÿ | 16GB+ |
| **Codestral** | Mistral | 22B | æ€§èƒ½ä¼˜ç§€ | 24GB+ |
| **StarCoder2** | BigCode | 3B - 15B | å¼€æºè®­ç»ƒ | 12GB+ |
| **Qwen-Coder** | é˜¿é‡Œ | 1.5B - 14B | ä¸­æ–‡ä¼˜åŒ– | 12GB+ |
| **CodeGemma** | Google | 2B - 7B | è½»é‡çº§ | 8GB+ |

---

## ğŸš€ DeepSeek Coder

### åŸºæœ¬ä¿¡æ¯
- **GitHub**: https://github.com/deepseek-ai/deepseek-coder
- **å®˜ç½‘**: https://www.deepseek.com/
- **è®¸å¯è¯**: Apache 2.0ï¼ˆå¯å•†ç”¨ï¼‰
- **å‚æ•°**: 6.7B, 20B, 33B
- **ä¸Šä¸‹æ–‡**: 16K tokens

### ç‰¹è‰²
âœ… **ä¸­æ–‡å‹å¥½**: ä¸“ä¸ºä¸­æ–‡ä¼˜åŒ–
âœ… **å¡«ç©ºèƒ½åŠ›**: FIMï¼ˆFill-In-Middleï¼‰æ¨¡å¼
âœ… **å¤šè¯­è¨€**: Python, JavaScript, C++, Go, Rust ç­‰
âœ… **å•†ä¸šå‹å¥½**: Apache 2.0 è®¸å¯è¯

### å¿«é€Ÿå¼€å§‹

#### ä½¿ç”¨ Ollama
```bash
# å®‰è£… Ollama
curl -fsSL https://ollama.com/install.sh | sh

# æ‹‰å–æ¨¡å‹
ollama pull deepseek-coder:6.7b

# è¿è¡Œ
ollama run deepseek-coder:6.7b
```

#### ä½¿ç”¨ Python API
```python
from openai import OpenAI

# è¿æ¥åˆ°æœ¬åœ° Ollama
client = OpenAI(
    base_url='http://localhost:11434/v1',
    api_key='ollama'  # required but unused
)

# ä»£ç ç”Ÿæˆ
response = client.chat.completions.create(
    model='deepseek-coder:6.7b',
    messages=[
        {
            'role': 'user',
            'content': 'ç”¨ Python å†™ä¸€ä¸ªå¿«é€Ÿæ’åº'
        }
    ]
)

print(response.choices[0].message.content)
```

### æ€§èƒ½å»ºè®®
- **6.7B**: é€‚åˆæ—¥å¸¸ç¼–ç¨‹ï¼Œéœ€è¦ 16GB RAM
- **33B**: æ›´å¼ºçš„æ¨ç†èƒ½åŠ›ï¼Œéœ€è¦ 64GB+ RAM

---

## ğŸ¦™ CodeLlama

### åŸºæœ¬ä¿¡æ¯
- **GitHub**: https://github.com/facebookresearch/codellama
- **å¼€å‘è€…**: Meta
- **è®¸å¯è¯**: Llama 2 Community
- **å‚æ•°**: 7B, 13B, 34B
- **ä¸Šä¸‹æ–‡**: up to 100K tokensï¼ˆ34B ç‰ˆæœ¬ï¼‰

### ç‰¹è‰²
âœ… **æˆç†Ÿç¨³å®š**: Meta å®˜æ–¹æ”¯æŒ
âœ… **é•¿ä¸Šä¸‹æ–‡**: æ”¯æŒ 16K-100K tokens
âœ… **Python ä¸“ç”¨**: æœ‰ä¸“é—¨é’ˆå¯¹ Python çš„ç‰ˆæœ¬
âœ… **å¡«ç©ºæ¨¡å¼**: æ”¯æŒ FIM

### å¿«é€Ÿå¼€å§‹

#### ä½¿ç”¨ Ollama
```bash
# æ‹‰å–æ¨¡å‹
ollama pull codellama:7b
ollama pull codellama:13b
ollama pull codellama:34b

# Python ä¸“ç”¨ç‰ˆæœ¬
ollama pull codellama:python
```

#### ä½¿ç”¨ Transformers
```python
from transformers import AutoTokenizer, AutoModelForCausalLM

# åŠ è½½æ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained("codellama/CodeLlama-7b-Python-hf")
model = AutoModelForCausalLM.from_pretrained("codellama/CodeLlama-7b-Python-hf")

# ç”Ÿæˆä»£ç 
prompt = "def fibonacci(n):"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_length=200)
code = tokenizer.decode(outputs[0])
print(code)
```

### ç‰ˆæœ¬é€‰æ‹©
- **CodeLlama-7b**: æ—¥å¸¸å¼€å‘ï¼Œ12GB RAM
- **CodeLlama-13b**: å¹³è¡¡æ€§èƒ½å’Œèµ„æºï¼Œ24GB RAM
- **CodeLlama-34b**: æœ€ä½³æ€§èƒ½ï¼Œ64GB+ RAM
- **CodeLlama-Python**: Python ä¸“ç”¨ï¼Œæ€§èƒ½æ›´å¥½

---

## ğŸŒŸ Codestral (Mistral)

### åŸºæœ¬ä¿¡æ¯
- **GitHub**: https://github.com/mistralai/mistral-src
- **å®˜ç½‘**: https://mistral.ai/
- **è®¸å¯è¯**: Mistral AI Licenseï¼ˆå¯å•†ç”¨ï¼‰
- **å‚æ•°**: 22B
- **ä¸Šä¸‹æ–‡**: 32K tokens

### ç‰¹è‰²
âœ… **æ€§èƒ½ä¼˜å¼‚**: åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²
âœ… **æ¨ç†èƒ½åŠ›å¼º**: æ•°å­¦é—®é¢˜è§£å†³
âœ… **å¤šè¯­è¨€**: æ”¯æŒ 80+ ç¼–ç¨‹è¯­è¨€
âœ… **å¿«é€Ÿ**: Mistral çš„æ¨ç†ä¼˜åŒ–

### å¿«é€Ÿå¼€å§‹

#### ä½¿ç”¨ Ollama
```bash
ollama pull codestral
```

#### ä½¿ç”¨ vLLM
```bash
pip install vllm

# å¯åŠ¨æœåŠ¡
python -m vllm.entrypoints.openai.api_server \
    --model mistralai/Codestral-22B-v0.1 \
    --host 0.0.0.0 \
    --port 8000
```

### é€‚ç”¨åœºæ™¯
- éœ€è¦é«˜æ€§èƒ½æ¨ç†
- æ•°å­¦å¯†é›†å‹ç¼–ç¨‹
- å¤æ‚ç®—æ³•å®ç°

---

## ğŸŒŸ StarCoder2

### åŸºæœ¬ä¿¡æ¯
- **GitHub**: https://github.com/bigcode-project/starcoder2
- **å®˜ç½‘**: https://huggingface.co/bigcode
- **å¼€å‘è€…**: BigCodeï¼ˆHugging Face + ServiceNowï¼‰
- **è®¸å¯è¯**: OpenRAIL-M
- **å‚æ•°**: 3B, 7B, 15B

### ç‰¹è‰²
âœ… **å®Œå…¨å¼€æº**: OpenRAIL è®¸å¯
âœ… **é€æ˜è®­ç»ƒ**: è®­ç»ƒæ•°æ®å…¬å¼€
âœ… **è½»é‡çº§**: 3B ç‰ˆæœ¬å¯åœ¨ CPU è¿è¡Œ
âœ… **å¤šè¯­è¨€**: æ”¯æŒ 358+ ç¼–ç¨‹è¯­è¨€

### å¿«é€Ÿå¼€å§‹

#### ä½¿ç”¨ Hugging Face
```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained("bigcode/starcoder2-7b")
tokenizer = AutoTokenizer.from_pretrained("bigcode/starcoder2-7b")

# ç”Ÿæˆä»£ç 
inputs = tokenizer("def hello_world():", return_tensors="pt")
outputs = model.generate(**inputs, max_length=100)
print(tokenizer.decode(outputs[0]))
```

### ç‰ˆæœ¬é€‰æ‹©
- **3B**: CPU å¯è¿è¡Œï¼Œ8GB RAM
- **7B**: GPU æ¨èï¼Œ16GB RAM
- **15B**: æœ€ä½³æ€§èƒ½ï¼Œ32GB+ RAM

---

## ğŸ‡¨ğŸ‡³ Qwen-Coder (é€šä¹‰åƒé—®)

### åŸºæœ¬ä¿¡æ¯
- **GitHub**: https://github.com/QwenLM/Qwen2.5-Coder
- **å¼€å‘è€…**: é˜¿é‡Œäº‘
- **è®¸å¯è¯**: Apache 2.0
- **å‚æ•°**: 0.5B, 1.5B, 3B, 7B, 14B

### ç‰¹è‰²
âœ… **ä¸­æ–‡ä¼˜åŒ–**: é’ˆå¯¹ä¸­æ–‡åœºæ™¯ä¼˜åŒ–
âœ… **è½»é‡çº§**: 0.5B ç‰ˆæœ¬å¯åœ¨ç§»åŠ¨ç«¯è¿è¡Œ
âœ… **å•†ä¸šå‹å¥½**: Apache 2.0 è®¸å¯
âœ… **é•¿ä¸Šä¸‹æ–‡**: æ”¯æŒ 32K tokens

### å¿«é€Ÿå¼€å§‹

#### ä½¿ç”¨ Ollama
```bash
ollama pull qwen2.5-coder:7b
```

#### ä½¿ç”¨ Transformers
```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-Coder-7B-Instruct",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-Coder-7B-Instruct")

# å¯¹è¯
messages = [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªç¼–ç¨‹åŠ©æ‰‹"},
    {"role": "user", "content": "ç”¨ Python å®ç°äºŒåˆ†æŸ¥æ‰¾"}
]

text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
inputs = tokenizer([text], return_tensors="pt")
outputs = model.generate(**inputs, max_length=512)
print(tokenizer.decode(outputs[0]))
```

---

## ğŸ”§ éƒ¨ç½²å·¥å…·

### Ollamaï¼ˆæœ€ç®€å•ï¼‰

#### ç‰¹ç‚¹
- ä¸€é”®å®‰è£…å’Œä½¿ç”¨
- è‡ªåŠ¨ç¡¬ä»¶ä¼˜åŒ–
- æ”¯æŒå¤šç§æ¨¡å‹
- è·¨å¹³å°ï¼ˆmacOS, Linux, Windowsï¼‰

#### å®‰è£…
```bash
# macOS / Linux
curl -fsSL https://ollama.com/install.sh | sh

# è¿è¡Œæ¨¡å‹
ollama run codellama:7b
```

#### API ä½¿ç”¨
```bash
# å¯åŠ¨ API æœåŠ¡
ollama serve

# è°ƒç”¨ API
curl http://localhost:11434/api/generate -d '{
  "model": "codellama:7b",
  "prompt": "å†™ä¸€ä¸ª Python å‡½æ•°"
}'
```

### vLLMï¼ˆé«˜æ€§èƒ½ï¼‰

#### ç‰¹ç‚¹
- é«˜ååé‡æ¨ç†
- PagedAttention æŠ€æœ¯
- OpenAI å…¼å®¹ API
- é€‚åˆç”Ÿäº§ç¯å¢ƒ

#### å®‰è£…å’Œä½¿ç”¨
```bash
# å®‰è£…
pip install vllm

# å¯åŠ¨æœåŠ¡
python -m vllm.entrypoints.openai.api_server \
    --model bigcode/starcoder2-7b \
    --host 0.0.0.0 \
    --port 8000

# è°ƒç”¨
curl http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "bigcode/starcoder2-7b",
    "prompt": "def hello():"
  }'
```

### LM Studioï¼ˆå›¾å½¢ç•Œé¢ï¼‰

#### ç‰¹ç‚¹
- å‹å¥½çš„ GUI
- æ¨¡å‹å¸‚åœº
- æœ¬åœ°èŠå¤©
- æ”¯æŒ GPU åŠ é€Ÿ

#### ä¸‹è½½
- https://lmstudio.ai/

### Text Generation WebUIï¼ˆåŠŸèƒ½ä¸°å¯Œï¼‰

#### ç‰¹ç‚¹
- Web ç•Œé¢
- æ”¯æŒå¤šç§åç«¯
- æ‰©å±•æ€§å¼º
- ç¤¾åŒºæ´»è·ƒ

#### å®‰è£…
```bash
git clone https://github.com/oobabooga/text-generation-webui
cd text-generation-webui
bash start_linux.sh  # æˆ– start_windows.bat / start_macos.sh
```

---

## ğŸ’» ç¡¬ä»¶éœ€æ±‚æŒ‡å—

### æœ€ä½é…ç½®ï¼ˆCPU æ¨ç†ï¼‰
- **CPU**: 8 æ ¸å¿ƒä»¥ä¸Š
- **RAM**: 16GB
- **å­˜å‚¨**: 50GB SSD
- **æ¨¡å‹**: CodeGemma-2B, StarCoder2-3B, Qwen-0.5B/1.5B
- **é€Ÿåº¦**: è¾ƒæ…¢ï¼ˆ2-10 tokens/ç§’ï¼‰

### æ¨èé…ç½®ï¼ˆGPU æ¨ç†ï¼‰
- **GPU**: RTX 3060 (12GB) æˆ–æ›´é«˜
- **RAM**: 32GB
- **å­˜å‚¨**: 100GB SSD
- **æ¨¡å‹**: CodeLlama-7B/13B, DeepSeek-6.7B/20B
- **é€Ÿåº¦**: å¿«ï¼ˆ20-50 tokens/ç§’ï¼‰

### æœ€ä½³é…ç½®ï¼ˆé«˜ç«¯ï¼‰
- **GPU**: RTX 4090 (24GB) æˆ– A100 (40GB+)
- **RAM**: 64GB+
- **å­˜å‚¨**: 200GB NVMe SSD
- **æ¨¡å‹**: CodeLlama-34B, DeepSeek-33B, Codestral
- **é€Ÿåº¦**: éå¸¸å¿«ï¼ˆ50-100+ tokens/ç§’ï¼‰

---

## ğŸ”— é›†æˆåˆ°å¼€å‘å·¥å…·

### VS Code + Continue

1. å®‰è£… Continue æ‰©å±•
2. é…ç½®ä½¿ç”¨ Ollama:

```json
{
  "models": [{
    "title": "DeepSeek Coder",
    "provider": "ollama",
    "model": "deepseek-coder:6.7b",
    "apiBase": "http://localhost:11434"
  }]
}
```

### Cursor + Ollama

1. æ‰“å¼€ Cursor è®¾ç½®
2. é…ç½®è‡ªå®šä¹‰æ¨¡å‹:
```
Model Provider: OpenAI Compatible
Base URL: http://localhost:11434/v1
API Key: ollama
Model: deepseek-coder:6.7b
```

### Aider + æœ¬åœ°æ¨¡å‹

```bash
# ä½¿ç”¨ Ollama æ¨¡å‹
aider --model ollama/deepseek-coder:6.7b

# ä½¿ç”¨ vLLM
aider --model openai/deepseek \
  --openai-api-base http://localhost:8000/v1 \
  --openai-api-key not-needed
```

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

### ä»£ç ç”ŸæˆåŸºå‡†ï¼ˆHumanEvalï¼‰

| æ¨¡å‹ | Pass@1 | Pass@10 |
|------|--------|---------|
| GPT-4 | 67.0% | 85.0% |
| Claude 3.5 | 72.0% | 88.0% |
| **DeepSeek-33B** | 62.0% | 78.0% |
| **Codestral-22B** | 65.0% | 81.0% |
| **CodeLlama-34B** | 48.8% | 60.0% |
| **StarCoder2-15B** | 45.0% | 58.0% |

### å®ç”¨å»ºè®®
- **æœ€ä½³è´¨é‡**: DeepSeek-33B, Codestral-22B
- **æœ€ä½³æ€§ä»·æ¯”**: DeepSeek-6.7B, CodeLlama-7B
- **æœ€è½»é‡**: Qwen-1.5B, CodeGemma-2B

---

## ğŸ¯ é€‰æ‹©å»ºè®®

### æŒ‰ç”¨é€”é€‰æ‹©

#### æ—¥å¸¸ç¼–ç¨‹
- **æ¨è**: DeepSeek Coder 6.7B
- **åŸå› **: ä¸­æ–‡å‹å¥½ï¼Œæ€§èƒ½å¥½ï¼Œèµ„æºéœ€æ±‚é€‚ä¸­

#### Python ä¸“ç”¨
- **æ¨è**: CodeLlama-Python 7B/13B
- **åŸå› **: é’ˆå¯¹ Python ä¼˜åŒ–

#### ä½ç«¯ç¡¬ä»¶
- **æ¨è**: Qwen-1.5B, CodeGemma-2B
- **åŸå› **: èµ„æºéœ€æ±‚ä½ï¼Œå¯åœ¨ CPU è¿è¡Œ

#### é«˜æ€§èƒ½éœ€æ±‚
- **æ¨è**: DeepSeek-33B, Codestral-22B
- **åŸå› **: æ¥è¿‘ GPT-3.5 æ°´å¹³

#### å•†ä¸šä½¿ç”¨
- **æ¨è**: DeepSeek, Qwen, Codestral
- **åŸå› **: å•†ä¸šå‹å¥½è®¸å¯è¯

### æŒ‰è¯­è¨€é€‰æ‹©

| è¯­è¨€ | æ¨èæ¨¡å‹ |
|------|---------|
| **Python** | CodeLlama-Python, DeepSeek |
| **JavaScript/TypeScript** | Codestral, StarCoder2 |
| **Go** | DeepSeek, StarCoder2 |
| **Rust** | Codestral, DeepSeek |
| **Java** | CodeLlama, StarCoder2 |
| **C++** | Codestral, CodeLlama-34B |
| **ä¸­æ–‡é¡¹ç›®** | DeepSeek, Qwen |

---

## ğŸ“š å­¦ä¹ èµ„æº

### å®˜æ–¹æ–‡æ¡£
- [Ollama æ–‡æ¡£](https://github.com/ollama/ollama/blob/main/docs.md)
- [vLLM æ–‡æ¡£](https://docs.vllm.ai/)
- [Hugging Face Transformers](https://huggingface.co/docs/transformers)

### æ•™ç¨‹
- [æœ¬åœ°è¿è¡Œ CodeLlama](https://www.philschmid.de/deploy-code-llama)
- [DeepSeek Coder ä½¿ç”¨æŒ‡å—](https://github.com/deepseek-ai/deepseek-coder)
- [Fine-tuning æ•™ç¨‹](https://huggingface.co/blog/lora-for-code)

### ç¤¾åŒº
- [Reddit r/LocalLLM](https://reddit.com/r/LocalLLM)
- [Hugging Face Forums](https://discuss.huggingface.co/)
- [Ollama Discord](https://discord.gg/ollama)

---

## ğŸ”— ç›¸å…³èµ„æº

- [AI å·¥å…·æ¸…å•](./ai-tools.md)
- [Agent æ¡†æ¶è¯¦è§£](./agent-frameworks.md)
- [GitHub ä»“åº“æ¨è](./github-repositories.md)
- [Ollama æ¨¡å‹åº“](https://ollama.com/library)

---

**æœ€åæ›´æ–°**: 2025-01-04
**ç»´æŠ¤è€…**: vibe-coding-collection ç¤¾åŒº
